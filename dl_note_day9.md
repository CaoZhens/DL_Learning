### Day9: Learning Note of Deep neural networks (Video 1-4)
> 备注：今天的内容主要包括：深层神经网络的符号约定、正向传播、向量化、矩阵维度

#### 深层神经网络的符号约定  
浅层NN与深层NN的符号约定是一样的，只是层数多少的区别；本节以复习为主。  
1. 神经网络的层数：从左到右，由0开始定义
2. 讨论神经网络的层数时，一般不考虑输入层，只考虑隐藏层和输出层

符号约定，以下图神经网络为例：  
<img src="https://github.com/CaoZhens/DL_Learning/blob/master/pic/deep_nn_picture.png" alt="" data-canonical-src="" width="400" height="200" />  

层数$L=4$  
第一个隐藏层$n^{[1]}=5$,表示有5个隐藏神经元  
同理$n^{[2]}=5, n^{[3]}=3, n^{[4]}=n^{[L]}=1$（输出单元为1)  
而输入层$n^{[0]}=n_x=3$  

#### 深层神经网络的正向传播
单样本：  
$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$
​$$a^{[l]} = g^{[l]}(z^{[l]})$$
向量化：
$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$
​$$A^{[l]} = g^{[l]}(Z^{[l]})$$

#### 核对矩阵的维度
单样本：  
$$w^{[l]}.shape = (n^{[l]}, n^{[l-1]})$$
$$b^{[l]}.shape = (n^{[l]}, 1)$$
$$z^{[l]}.shape = (n^{[l]}, 1)$$
$$a^{[l]}.shape = (n^{[l]}, 1)$$  
向量化：  
$$w^{[l]}.shape = (n^{[l]}, n^{[l-1]})$$
$$b^{[l]}.shape = (n^{[l]}, 1)$$
$$Z^{[l]}.shape = (n^{[l]}, m)$$
$$A^{[l]}.shape = (n^{[l]}, m)$$  