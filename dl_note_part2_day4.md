# Day4: 深度学习的实用层面(Practical aspects of Deep Learning) Video10-14
> 今天的主要学习内容包括：梯度消失与梯度爆炸；神经网络的权重初始化；梯度检验

## 梯度消失 & 梯度爆炸
训练神经网络的时候，有可能遇到导数变得非常大或者非常小的情况，这加大了训练的难度。  
例子：  
假设正在训练一个很深的神经网络，包含参数$\mathbf{W}^{[1]}, \mathbf{W}^{[2]}, ...,\mathbf{W}^{[l]}$  
假设使用线性激活函数：$g(z) = z$，并忽略$b$（$b^{[l]}=0$）  
则：  
$$\hat{y} = \mathbf{W}^{[l]}\mathbf{W}^{[l-1]}\mathbf{W}^{[l-2]}\cdots\mathbf{W}^{[2]}\mathbf{W}^{[1]}x$$

假设权重矩阵
$$\mathbf{W}^{[l]} = \left[
    \begin{matrix}
      1.5 & 0 \\  
      0 & 1.5
    \end{matrix}
  \right]$$
从技术上讲，最后一层的权重矩阵可能有不同维度（如何理解？）  
$$\hat{y} = \mathbf{W}^{[l]}{\left[
    \begin{matrix}
      1.5 & 0 \\  
      0 & 1.5
    \end{matrix}
  \right]}^{l-1}x$$
即：对于一个深度神经网络来说如果$L$值较大，那么$\hat{y}$的值也会非常大，呈指数级增长；  
相反，如果权重矩阵是：  
$$\mathbf{W}^{[l]} = \left[
    \begin{matrix}
      0.5 & 0 \\  
      0 & 0.5
    \end{matrix}
  \right]$$
激活函数的值将以指数级下降。  
**直观理解：**  
当权重W​只比1略大一点，或者说只是比单位矩阵大一点，深度神经网络的激活函数将爆炸式增长，如果W​比1略小一点，在深度神经网络中，激活函数将以指数级递减，虽然这里只是讨论了激活函数以与L相关的指数级数增长或下降，它也适用于与层数L相关的导数或梯度函数，也是呈指数级增长或呈指数递减。  
在深度神经网络中，激活函数将以指数级递减，虽然我只是讨论了激活函数以与L相关的指数级数增长或下降，它也适用于与层数L相关的导数或梯度函数，也是呈指数级增长或呈指数递减。

不妨假设$L > 100$，在这样一个非常深的神经网络中，如果激活函数或梯度函数以与L相关的指数增长或递减，它们的值将会变得极大或极小，从而导致训练难度上升，尤其是梯度指数小于L时，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习。

## 神经网络的权重初始化
首先用一个神经网络单元初始化的例子：  
假设有4个输入特征，从$x_1$到$x_4$，$b=0$：  
$$z = w_1x_1 + w_2x_2 + \cdots + w_nx_n$$
为了防止$z$过大或过小，当$n$越大时，希望$w_i$越小，即设置：
$$w_i = \frac1n$$
实际上，在神经网络中可以设置某层的权重矩阵为：  
$$\mathbf{W}^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}})$$

## 梯度检验（梯度的数值逼近）
$$f^{\prime}(\theta) = \frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}$$
使用双边误差来判断函数f的偏导是否正确，从而检验反向传播是否得以正确实施，如果不正确，它可能有bug需要你来解决。    

注意事项：  
- 不要在训练中使用梯度检验，它只用于调试
- 如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出bug
- 在实施梯度检验时，如果使用正则化，请注意要包括正则项
- 梯度检验不能与Dropout同时使用