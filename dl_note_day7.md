### Day7: Learning Note of Shallow neural networks (Video 5-8)
> 备注：今天的内容主要包括：对向量化的解释、激活函数

#### 对向量化的解释
对单个样本的计算：
$$z^{[1]} = w^{[1]}x^{(i)} + b^{[1]}$$
扩展：有多个训练样本时，将它们堆到矩阵$\mathbf{X}$的各列中，那么它们的输出也就会相应的堆叠到矩阵$\mathbf{Z}^{[1]}$的各列中。  
关于向量化的总结：
1. 向量化不需要显示循环，直接通过矩阵运算，从$\mathbf{X}$​就可以计算出 $\mathbf{A}^{[1]}$​(实际上$\mathbf{X}$可以记为$\mathbf{A}^{[0]}$)。
2. 使用同样的方法就可以由神经网络中的每一层的输入$\mathbf{A}^{[i-1]}$计算输出$\mathbf{A}^{[i]}$；即神经网络的不同层次的每一步操作都一样，只不过是同样的计算过程不断重复而已。

#### 激活函数
**使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。** 前面我们一直在使用sigmoid函数；**但其它的激活函数效果也许更好。**  
tanh函数：
$$a = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

ReLu 修正线性单元函数：
$$ a = \max(0, z)$$

Leaky Relu:
$$ a = \max(0.01z, z)$$

**总结：**
1. sigmoid激活函数：除了输出层是一个二分类问题基本不会用它。
2. tanh激活函数：tanh是非常优秀的，几乎适合所有场合。
3. ReLu激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用ReLu或者Leaky ReLu。

#### 为什么需要非线性激活函数
实质：因为n个线性函数的组合本身仍然时线性函数；**如果使用线性激活函数，无论神经网络有多少层，一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。**

#### 几个典型激活函数的导数
sigmoid函数：  
$$\sigma{(z)}^\prime = \left( \frac{1}{1+e^{-z}}\right)^\prime = \frac{-1}{(1+e^{-z})^2} e^{-z}(-1) = \frac{1}{(1+e^{-z})} \frac{e^{-z}}{(1+e^{-z})} = \sigma{(z)}(1-\sigma{(z)})$$

tanh函数：
$$\tanh(z)^\prime = \frac{(e^z+e^{-z})(e^z+e^{-z})-(e^z-e^{-z})(e^z-e^{-z})}{(e^z+e^{-z})^2} = 1 - (\tanh(z))^2$$